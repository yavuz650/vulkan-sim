{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from multiprocessing import Process, Queue , Array\n",
    "import pickle\n",
    "\n",
    "class RayData:\n",
    "  def __init__(self, tid, ray_orig_x, ray_orig_y, ray_orig_z, ray_dir_x, ray_dir_y, ray_dir_z):\n",
    "    self.tid = tid\n",
    "\n",
    "    self.ray_orig_x=ray_orig_x\n",
    "    self.ray_orig_y=ray_orig_y\n",
    "    self.ray_orig_z=ray_orig_z\n",
    "\n",
    "    self.ray_dir_x=ray_dir_x\n",
    "    self.ray_dir_y=ray_dir_y\n",
    "    self.ray_dir_z=ray_dir_z\n",
    "\n",
    "  def __eq__(self, other):\n",
    "    if isinstance(other, RayData):\n",
    "      # Compare attributes for equality\n",
    "      return  self.tid == other.tid and \\\n",
    "              self.ray_orig_x == other.ray_orig_x and \\\n",
    "              self.ray_orig_y == other.ray_orig_y and \\\n",
    "              self.ray_orig_z == other.ray_orig_z and \\\n",
    "              self.ray_dir_x == other.ray_dir_x and \\\n",
    "              self.ray_dir_y == other.ray_dir_y and \\\n",
    "              self.ray_dir_z == other.ray_dir_z\n",
    "    return False\n",
    "  \n",
    "  def __str__(self):\n",
    "    # Customize the string representation for printing\n",
    "    return f\"ray_data instance: {self.tid}, {self.ray_orig_x}, {self.ray_orig_y}, {self.ray_orig_z}, {self.ray_dir_x}, {self.ray_dir_y}, {self.ray_dir_z}\"\n",
    "\n",
    "class MemEntry:\n",
    "  def __init__(self, address, size, type):\n",
    "    self.entry = [address, size, type]\n",
    "\n",
    "  def __eq__(self, other):\n",
    "    if isinstance(other, MemEntry):\n",
    "      # Compare attributes for equality\n",
    "      return self.entry[0] == other.entry[0] and self.entry[1] == other.entry[1]\n",
    "    return False\n",
    "  \n",
    "  def __str__(self):\n",
    "    # Customize the string representation for printing\n",
    "    return f\"mem_entry instance: {self.entry}\"\n",
    "\n",
    "class TraceRayEntry:\n",
    "  def __init__(self, ray_data: RayData, mem_entries: List[MemEntry]):\n",
    "    self.ray_data = ray_data\n",
    "    self.mem_entries = mem_entries\n",
    "\n",
    "  def __str__(self):\n",
    "    # Customize the string representation for printing\n",
    "    return f\"trace_ray_entry instance: {self.ray_data}, {self.mem_entries}\"\n",
    "\n",
    "class HashedRay:\n",
    "  def __init__(self, hash, tid: int, rayid: int):\n",
    "    self.hash = hash\n",
    "    self.tid = tid # thread id, index to the thread_list\n",
    "    self.rayid = rayid # ray id, index to the entry in the thread_list\n",
    "\n",
    "  def __str__(self):\n",
    "    return f\"HashedRay: hash:{self.hash}, tid:{self.tid}, rayid:{self.rayid}\"\n",
    "\n",
    "# Quantize direction to a sphere - xyz to theta and phi\n",
    "# `theta_bits` is used for theta, `theta_bits` + 1 is used for phi, for a total of\n",
    "# 2 * `theta_bits` + 1 bits\n",
    "def hash_direction_spherical(d, num_sphere_bits):\n",
    "  theta_bits = np.uint32(num_sphere_bits)\n",
    "  phi_bits = np.uint32(theta_bits + 1)\n",
    "\n",
    "  theta = np.uint64(np.arccos(np.clip(d[2], -1.0, 1.0)) / np.pi * 180)\n",
    "  phi = np.uint64((np.arctan2(d[1], d[0]) + np.pi) / np.pi * 180)\n",
    "  q_theta = theta >> np.uint64(8 - theta_bits)\n",
    "  q_phi = phi >> np.uint64(9 - phi_bits)\n",
    "\n",
    "  return (q_phi << theta_bits) | q_theta\n",
    "\n",
    "def hash_origin_grid(o, min_val, max_val, num_bits):\n",
    "  grid_size = 1 << num_bits\n",
    "\n",
    "  hash_o_x = np.uint64(np.clip((o[0] - min_val[0]) / (max_val[0] - min_val[0]) * grid_size, 0.0, float(grid_size) - 1))\n",
    "  hash_o_y = np.uint64(np.clip((o[1] - min_val[1]) / (max_val[1] - min_val[1]) * grid_size, 0.0, float(grid_size) - 1))\n",
    "  hash_o_z = np.uint64(np.clip((o[2] - min_val[2]) / (max_val[2] - min_val[2]) * grid_size, 0.0, float(grid_size) - 1))\n",
    "  \n",
    "  hash_value = (hash_o_x << np.uint32((2 * num_bits))) | (hash_o_y << np.uint32(num_bits)) | hash_o_z\n",
    "  return np.uint64(hash_value)\n",
    "\n",
    "def hash_grid_spherical(ray_direction, ray_origin, min_val, max_val, num_grid_bits, num_sphere_bits):\n",
    "  hash_d = hash_direction_spherical(ray_direction, num_sphere_bits)\n",
    "  hash_o = hash_origin_grid(ray_origin, min_val, max_val, num_grid_bits)\n",
    "  hash_value = hash_o ^ hash_d\n",
    "\n",
    "  return hash_value\n",
    "\n",
    "def parse_csv(csv, number_of_threads):\n",
    "  thread_list = [[] for _ in range(number_of_threads)]\n",
    "  mem_entries = []\n",
    "  last_idx = csv[0][Indices.IDX]\n",
    "  last_tid = csv[0][Indices.TID]\n",
    "  last_ray_data = RayData(csv[0][Indices.TID],\n",
    "                           csv[0][Indices.RAY_ORIG_X],\n",
    "                           csv[0][Indices.RAY_ORIG_Y],\n",
    "                           csv[0][Indices.RAY_ORIG_Z],\n",
    "                           csv[0][Indices.RAY_DIR_X],\n",
    "                           csv[0][Indices.RAY_DIR_Y],\n",
    "                           csv[0][Indices.RAY_DIR_Z])\n",
    "  print(\"Parsing the csv...\")\n",
    "  for entry in csv:\n",
    "    if(last_idx != entry[Indices.IDX]):\n",
    "      thread_list[last_tid].append(TraceRayEntry(last_ray_data,mem_entries[:]))\n",
    "      mem_entries.clear()\n",
    "      last_ray_data = RayData(entry[Indices.TID],\n",
    "                               entry[Indices.RAY_ORIG_X],\n",
    "                               entry[Indices.RAY_ORIG_Y],\n",
    "                               entry[Indices.RAY_ORIG_Z],\n",
    "                               entry[Indices.RAY_DIR_X],\n",
    "                               entry[Indices.RAY_DIR_Y],\n",
    "                               entry[Indices.RAY_DIR_Z])\n",
    "      last_idx = entry[Indices.IDX]\n",
    "      last_tid = entry[Indices.TID]\n",
    "    \n",
    "    mem_entries.append(MemEntry(entry[Indices.ADDR],entry[Indices.SIZE],entry[Indices.TYPE]))\n",
    "  print(\"Finished parsing the csv...\")\n",
    "  return thread_list\n",
    "\n",
    "class Indices:\n",
    "  IDX = 0\n",
    "  TID = 1\n",
    "  ADDR = 2\n",
    "  SIZE = 3\n",
    "  TYPE = 4\n",
    "  RAY_ORIG_X = 5\n",
    "  RAY_ORIG_Y = 6\n",
    "  RAY_ORIG_Z = 7\n",
    "  RAY_DIR_X = 8\n",
    "  RAY_DIR_Y = 9\n",
    "  RAY_DIR_Z = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_bunny_stereo():\n",
    "  print(\"Loading bunny\")\n",
    "  # Read the first CSV file into a DataFrame\n",
    "  df1 = pd.read_csv('../../outputs/bunny_left_eye_mem_access.csv')\n",
    "  # Convert the DataFrame to a list of lists\n",
    "  csv1 = df1.values.tolist()\n",
    "  number_of_threads1 = df1['tid'].nunique()\n",
    "\n",
    "  thread_list1 = parse_csv(csv1,number_of_threads1)\n",
    "\n",
    "  # Read the second CSV file into a DataFrame\n",
    "  df2 = pd.read_csv('../../outputs/bunny_right_eye_mem_access.csv')\n",
    "  # Convert the DataFrame to a list of lists\n",
    "  csv2 = df2.values.tolist()\n",
    "  number_of_threads2 = df2['tid'].nunique()\n",
    "\n",
    "  thread_list2 = parse_csv(csv2,number_of_threads2)\n",
    "\n",
    "  min_val = np.array([0.0, 0.0, -555.0])  # bunny min values\n",
    "  max_val = np.array([556.0, 556.0, 1.0])  # bunny max values\n",
    "\n",
    "  return thread_list1,thread_list2,min_val,max_val\n",
    "\n",
    "def load_sponza_stereo():\n",
    "  print(\"Loading sponza\")\n",
    "  # Read the first CSV file into a DataFrame\n",
    "  df1 = pd.read_csv('../../outputs/sponza_left_eye_mem_access.csv')\n",
    "  # Convert the DataFrame to a list of lists\n",
    "  csv1 = df1.values.tolist()\n",
    "  number_of_threads1 = df1['tid'].nunique()\n",
    "\n",
    "  thread_list1 = parse_csv(csv1,number_of_threads1)\n",
    "\n",
    "  # Read the second CSV file into a DataFrame\n",
    "  df2 = pd.read_csv('../../outputs/sponza_right_eye_mem_access.csv')\n",
    "  # Convert the DataFrame to a list of lists\n",
    "  csv2 = df2.values.tolist()\n",
    "  number_of_threads2 = df2['tid'].nunique()\n",
    "\n",
    "  thread_list2 = parse_csv(csv2,number_of_threads2)\n",
    "\n",
    "  min_val = np.array([-1105.42603,-126.442497,-1920.94592]) # sponza min values\n",
    "  max_val = np.array([1198.57397,1433.5575,1807.05408]) # sponza max values\n",
    "\n",
    "  return thread_list1,thread_list2,min_val,max_val\n",
    "\n",
    "def hash_rays(thread_list1, min_val, max_val, num_grid_bits, num_sphere_bits):\n",
    "  print(\"Hashing rays...\")\n",
    "  hash_list1 = []\n",
    "  for tid,thread in enumerate(thread_list1):\n",
    "    for rayid,ray in enumerate(thread):\n",
    "      # only hash if the ray intersects the scene\n",
    "      if(any(obj.entry[2] == 5 for obj in ray.mem_entries)):\n",
    "        hash_list1.append(HashedRay(hash_grid_spherical(np.array([ray.ray_data.ray_dir_x, ray.ray_data.ray_dir_y, ray.ray_data.ray_dir_z]),\n",
    "                                            np.array([ray.ray_data.ray_orig_x, ray.ray_data.ray_orig_y, ray.ray_data.ray_orig_z]),\n",
    "                                            min_val, max_val,num_grid_bits,num_sphere_bits), tid, rayid))\n",
    "  return hash_list1\n",
    "\n",
    "def hash_rays_random(thread_list1):\n",
    "  print(\"Hashing rays randomly...\")\n",
    "  hash_list1 = []\n",
    "  for tid,thread in enumerate(thread_list1):\n",
    "    for rayid,ray in enumerate(thread):\n",
    "      # only hash if the ray intersects the scene\n",
    "      if(any(obj.entry[2] == 5 for obj in ray.mem_entries)):\n",
    "        hash_list1.append(HashedRay(random.randint(0, 32678), tid, rayid))\n",
    "  \n",
    "  return hash_list1\n",
    "\n",
    "def check_matches(list1: List[MemEntry], list2: List[MemEntry]):\n",
    "  num_matches = 0\n",
    "  length = len(list2) if len(list2) < len(list1) else len(list1)\n",
    "  for idx in range(length):\n",
    "      if(list1[idx].entry[0] == list2[idx].entry[0] and list1[idx].entry[1] == list2[idx].entry[1]):\n",
    "        num_matches += 1\n",
    "      else:\n",
    "        break\n",
    "  \n",
    "  return num_matches\n",
    "\n",
    "# Find all matching hashes, counts the matching memory entries and averages them\n",
    "def find_all_matches_best(hash_list1, hash_list2, thread_list1, thread_list2, start, length, full_match_list,full_len_list,full_tid_list,full_rayid_list):\n",
    "  if start+length > len(hash_list2):\n",
    "    length = len(hash_list2) - start  \n",
    "  for idx,ray2 in enumerate(hash_list2[start:start+length]):\n",
    "    found = 0\n",
    "    num_matching_hashes = 1\n",
    "    total_matches = 0\n",
    "    mem_len = len(thread_list2[ray2.tid][ray2.rayid].mem_entries)\n",
    "    for ray1 in hash_list1:\n",
    "      if(ray1.hash == ray2.hash):\n",
    "        matches = check_matches(thread_list1[ray1.tid][ray1.rayid].mem_entries, thread_list2[ray2.tid][ray2.rayid].mem_entries)\n",
    "        num_matching_hashes += 1\n",
    "        total_matches += matches\n",
    "        #best_tid = ray1.tid\n",
    "        #best_rayid = ray1.rayid\n",
    "        found = 1\n",
    "      elif found:\n",
    "        break\n",
    "    if num_matching_hashes > 1:\n",
    "      num_matching_hashes -= 1\n",
    "    full_match_list[start+idx] = np.int32(np.ceil(total_matches/num_matching_hashes))\n",
    "    full_len_list[start+idx] = mem_len\n",
    "    #full_tid_list[idx] = best_tid\n",
    "    #full_rayid_list[idx] = best_rayid\n",
    "    # print(f\"hash: {str(ray2.hash).ljust(5)}, {str(best_matches).rjust(4)}/{str(best_len).ljust(4)}, \"\n",
    "    #       f\"tid1,rayid1: {str(best_tid).rjust(5)},{str(best_rayid).rjust(2)}, \"\n",
    "    #       f\"tid2,rayid2: {str(ray2.tid).rjust(5)},{str(ray2.rayid).rjust(2)}, \"\n",
    "    #       f\"raydata1: {thread_list1[best_tid][best_rayid].ray_data}, \" \n",
    "    #       f\"raydata2: {thread_list2[ray2.tid][ray2.rayid].ray_data}\")\n",
    "\n",
    "def find_all_matches_best_parallel(hash_list1, hash_list2, thread_list1, thread_list2, n=10):\n",
    "  full_length = len(hash_list2)\n",
    "  length = np.int32(np.ceil(full_length / n))\n",
    "\n",
    "  full_match_list = Array('i',full_length)\n",
    "  full_len_list = Array('i',full_length)\n",
    "  full_tid_list = Array('i',full_length)\n",
    "  full_rayid_list = Array('i',full_length)\n",
    "\n",
    "  procs = [[] for _ in range(n)]\n",
    "\n",
    "  for i in range(n):\n",
    "    procs[i] = Process(target=find_all_matches_best, args=(hash_list1,hash_list2,thread_list1,thread_list2,i*length,length,full_match_list,full_len_list,full_tid_list,full_rayid_list))\n",
    "    procs[i].start()\n",
    "  for i in range(n):\n",
    "    procs[i].join()\n",
    "\n",
    "  return np.array(full_match_list), np.array(full_len_list), np.array(full_tid_list), np.array(full_rayid_list)\n",
    "\n",
    "def find_all_matches_nearest(hash_list1, hash_list2, thread_list1, thread_list2, start, length,full_match_list,full_len_list,full_tid_list,full_rayid_list):\n",
    "  if start+length > len(hash_list2):\n",
    "    length = len(hash_list2) - start\n",
    "  print(f\"{start},{length}\")\n",
    "  for idx,ray2 in enumerate(hash_list2[start:start+length]):\n",
    "    nearest_matches = 0\n",
    "    nearest_len = len(thread_list2[ray2.tid][ray2.rayid].mem_entries)\n",
    "    nearest_tid = 0\n",
    "    nearest_rayid = 0\n",
    "    found = 0\n",
    "    min_distance = 32768\n",
    "    for ray1 in hash_list1:\n",
    "      if(ray1.hash == ray2.hash):\n",
    "        if abs(ray1.tid-ray2.tid) < min_distance:\n",
    "          min_distance = abs(ray1.tid-ray2.tid)\n",
    "          matches = check_matches(thread_list1[ray1.tid][ray1.rayid].mem_entries, thread_list2[ray2.tid][ray2.rayid].mem_entries)\n",
    "          nearest_matches = matches\n",
    "          nearest_tid = ray1.tid\n",
    "          nearest_rayid = ray1.rayid\n",
    "          found = 1\n",
    "      elif found:\n",
    "        break\n",
    "    full_match_list[start+idx] = nearest_matches\n",
    "    full_len_list[start+idx] = nearest_len\n",
    "    full_tid_list[start+idx] = nearest_tid\n",
    "    full_rayid_list[start+idx] = nearest_rayid\n",
    "\n",
    "def find_all_matches_nearest_parallel(hash_list1, hash_list2, thread_list1, thread_list2, n=10):\n",
    "  \n",
    "  full_length = len(hash_list2)\n",
    "  length = np.int32(np.ceil(full_length / n))\n",
    "\n",
    "  full_match_list = Array('i',full_length)\n",
    "  full_len_list = Array('i',full_length)\n",
    "  full_tid_list = Array('i',full_length)\n",
    "  full_rayid_list = Array('i',full_length)\n",
    "\n",
    "  procs = [[] for _ in range(n)]\n",
    "\n",
    "  for i in range(n):\n",
    "    procs[i] = Process(target=find_all_matches_nearest, args=(hash_list1,hash_list2,thread_list1,thread_list2,i*length,length,full_match_list,full_len_list,full_tid_list,full_rayid_list))\n",
    "    procs[i].start()\n",
    "  for i in range(n):\n",
    "    procs[i].join()\n",
    "\n",
    "  return np.array(full_match_list), np.array(full_len_list), np.array(full_tid_list), np.array(full_rayid_list)\n",
    "\n",
    "def dump_scene_threadlist_pickle(thread_list1,thread_list2,scene: str):\n",
    "  with open(f'{scene}_left_threadlist.pickle', 'wb') as f:\n",
    "    pickle.dump(thread_list1, f)\n",
    "   \n",
    "  with open(f'{scene}_right_threadlist.pickle', 'wb') as f:\n",
    "    pickle.dump(thread_list2, f)\n",
    "\n",
    "def dump_scene_hashlist_pickle(hash_list1,hash_list2,scene: str,hash_label: str):\n",
    "  with open(f'{scene}_left_{hash_label}_hashlist.pickle', 'wb') as f:\n",
    "    pickle.dump(hash_list1, f)\n",
    "\n",
    "  with open(f'{scene}_right_{hash_label}_hashlist.pickle', 'wb') as f:\n",
    "    pickle.dump(hash_list2, f)\n",
    "\n",
    "def load_scene_threadlist_pickle(scene: str):\n",
    "  with open(f'{scene}_left_threadlist.pickle', 'rb') as f:\n",
    "    thread_list1 = pickle.load(f)\n",
    "   \n",
    "  with open(f'{scene}_right_threadlist.pickle', 'rb') as f:\n",
    "    thread_list2 = pickle.load(f)\n",
    "\n",
    "  return thread_list1, thread_list2\n",
    "\n",
    "def load_scene_hashlist_pickle(scene: str, hash_label: str):\n",
    "  with open(f'{scene}_left_{hash_label}_hashlist.pickle', 'rb') as f:\n",
    "    hash_list1 = pickle.load(f)\n",
    "\n",
    "  with open(f'{scene}_right_{hash_label}_hashlist.pickle', 'rb') as f:\n",
    "    hash_list2 = pickle.load(f)\n",
    "\n",
    "  return hash_list1,hash_list2\n",
    "\n",
    "# # get rid of multiple entries by picking the entry\n",
    "# def compress_hash_list_last(hash_list):\n",
    "#   # sort by hashes first\n",
    "#   hash_list.sort(key=lambda x:x.hash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashing rays...\n",
      "Hashing rays...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# thread_list1,thread_list2,min_val,max_val = load_sponza_stereo()\n",
    "# hash_list1 = hash_rays(thread_list1,min_val,max_val,5,3)\n",
    "# hash_list2 = hash_rays(thread_list2,min_val,max_val,5,3)\n",
    "# dump_scene_pickle(thread_list1,thread_list2,hash_list1,hash_list2,\"sponza\")\n",
    "min_val = np.array([-1105.42603,-126.442497,-1920.94592]) # sponza min values\n",
    "max_val = np.array([1198.57397,1433.5575,1807.05408]) # sponza max values\n",
    "\n",
    "thread_list1,thread_list2 =  load_scene_threadlist_pickle(\"sponza\")\n",
    "hash_list1 = hash_rays(thread_list1,min_val,max_val,5,3)\n",
    "hash_list2 = hash_rays(thread_list2,min_val,max_val,5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,4823\n",
      "4823,4823\n",
      "9646,4823\n",
      "14469,4823\n",
      "19292,4823\n",
      "24115,4823\n",
      "28938,4823\n",
      "33761,4823\n",
      "38584,4823\n",
      "43407,4823\n",
      "48230,4823\n",
      "53053,4823\n",
      "57876,4823\n",
      "62699,4823\n",
      "67522,4812\n"
     ]
    }
   ],
   "source": [
    "m_near,l_near,t_near,r_near = find_all_matches_nearest_parallel(hash_list1, hash_list2, thread_list1, thread_list2, n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_best,l_best,t_best,r_best = find_all_matches_best_parallel(hash_list1, hash_list2, thread_list1, thread_list2, n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20796913857294094\n",
      "0.20797122839158388\n"
     ]
    }
   ],
   "source": [
    "z_near=m_near/l_near\n",
    "print(np.mean(z_near))\n",
    "z_best=m_best/l_best\n",
    "print(np.mean(z_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
